You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
/usr/local/lib/python3.10/dist-packages/apex/normalization/fused_layer_norm.py:214: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=False):
Model: t5 | Checkpoint: best_3b_100k.pt | Device: cuda
Evaluating on 200 samples...
  Processed 100/200
  Processed 200/200

==================================================
Model: t5 | Checkpoint: best_3b_100k.pt
==================================================
BLEU:   2.52
BLEU-1: 23.62
BLEU-2: 3.52
BLEU-3: 1.42
BLEU-4: 0.86
==================================================
Avg Latency: 711.27 ms/sentence
Total Time:  142.25 s
Throughput:  1.41 sentences/s
==================================================
