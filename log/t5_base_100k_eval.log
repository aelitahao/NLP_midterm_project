You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
/usr/local/lib/python3.10/dist-packages/apex/normalization/fused_layer_norm.py:214: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=False):
Model: t5 | Checkpoint: best_base_100k.pt | Device: cuda
Evaluating on 200 samples...
  Processed 100/200
  Processed 200/200

==================================================
Model: t5 | Checkpoint: best_base_100k.pt
==================================================
BLEU:   2.54
BLEU-1: 24.77
BLEU-2: 4.07
BLEU-3: 1.66
BLEU-4: 1.02
==================================================
Avg Latency: 362.58 ms/sentence
Total Time:  72.52 s
Throughput:  2.76 sentences/s
==================================================
