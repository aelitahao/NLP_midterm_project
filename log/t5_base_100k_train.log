You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
'(ReadTimeoutError("HTTPSConnectionPool(host='cas-bridge.xethub.hf.co', port=443): Read timed out. (read timeout=10)"), '(Request ID: 1f940b92-3a8e-48fe-a25e-7bcae4ee9ec5)')' thrown while requesting GET https://cas-bridge.xethub.hf.co/xet-bridge-us/621ffdc036468d709f174356/d56a0dd1b70da7dd7d680239df56a5cf53543d81ab4d92a3584bf673648d7684?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20251225%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20251225T112348Z&X-Amz-Expires=3600&X-Amz-Signature=db816a2edbc8601333f43b2e55570c46e559d4f1bc5dd09d991d3c5c7ef3a74d&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=62171e3b6a99db28e0b3159d&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27model.safetensors%3B+filename%3D%22model.safetensors%22%3B&x-id=GetObject&Expires=1766665428&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc2NjY2NTQyOH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82MjFmZmRjMDM2NDY4ZDcwOWYxNzQzNTYvZDU2YTBkZDFiNzBkYTdkZDdkNjgwMjM5ZGY1NmE1Y2Y1MzU0M2Q4MWFiNGQ5MmEzNTg0YmY2NzM2NDhkNzY4NCoifV19&Signature=aUv7o6yyicH5%7Em4Eb0BNc6p2W15Zy4w9jF7Q308JmeRlvwuXaHUq1DdIssVPj7zEs7rLAfZksgM68m4fTGp5MjzV%7EyuE3e9qKTDrxgVDksiDBUElvrOwbj9e-iD5C3GYMsqk4yrsCwZQiDi-rt6qIX8NFwcOiH43jU06foQyQU%7EJm7CJivTKkE-dBZHZCJEZLYAkvBW8ms2LwFjMu6rMdxYhJz-iYYABZO-WuFz1gkkxZhddSJUAJf%7ElOmyYrqpMJjNP%7EQlbZrcAn1VaL6c9VxQBIYbwviEC97lhxedyFUtlyj54zmMA315POmA4dq94rB3oRp5AxKDRiZNzUY5vSw__&Key-Pair-Id=K2L8F4GPSG1IFC
Retrying in 1s [Retry 1/5].
Initializing t5-base on cuda...
Running full-parameter fine-tuning.
Loading data from data/raw/train_100k.jsonl...
Loaded 100000 samples.
Loading data from data/raw/valid.jsonl...
Loaded 500 samples.

Start Training: 100k | Effective Batch Size: 64 | AMP: bf16
/usr/local/lib/python3.10/dist-packages/apex/_autocast_utils.py:26: FutureWarning: `torch.cuda.amp.autocast_mode._cast(value, dtype)` is deprecated. Please use `torch.amp.autocast_mode._cast(value, 'cuda', dtype)` instead.
  return torch.cuda.amp.autocast_mode._cast(args, torch.get_autocast_gpu_dtype())
/usr/local/lib/python3.10/dist-packages/apex/normalization/fused_layer_norm.py:214: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=False):
Epoch 0 | Batch 0/1563 | Loss: 4.0657
Epoch 0 | Batch 50/1563 | Loss: 3.4979
Epoch 0 | Batch 100/1563 | Loss: 3.4848
Epoch 0 | Batch 150/1563 | Loss: 3.4189
Epoch 0 | Batch 200/1563 | Loss: 3.3446
Epoch 0 | Batch 250/1563 | Loss: 3.3720
Epoch 0 | Batch 300/1563 | Loss: 3.2922
Epoch 0 | Batch 350/1563 | Loss: 3.3392
Epoch 0 | Batch 400/1563 | Loss: 3.2840
Epoch 0 | Batch 450/1563 | Loss: 3.3569
Epoch 0 | Batch 500/1563 | Loss: 3.4382
Epoch 0 | Batch 550/1563 | Loss: 3.3349
Epoch 0 | Batch 600/1563 | Loss: 3.3286
Epoch 0 | Batch 650/1563 | Loss: 3.2633
Epoch 0 | Batch 700/1563 | Loss: 3.2713
Epoch 0 | Batch 750/1563 | Loss: 3.3479
Epoch 0 | Batch 800/1563 | Loss: 3.2793
Epoch 0 | Batch 850/1563 | Loss: 3.2994
Epoch 0 | Batch 900/1563 | Loss: 3.2307
Epoch 0 | Batch 950/1563 | Loss: 3.4374
Epoch 0 | Batch 1000/1563 | Loss: 3.3248
Epoch 0 | Batch 1050/1563 | Loss: 3.1681
Epoch 0 | Batch 1100/1563 | Loss: 3.1479
Epoch 0 | Batch 1150/1563 | Loss: 3.2784
Epoch 0 | Batch 1200/1563 | Loss: 3.2869
Epoch 0 | Batch 1250/1563 | Loss: 3.2571
Epoch 0 | Batch 1300/1563 | Loss: 3.2506
Epoch 0 | Batch 1350/1563 | Loss: 3.3411
Epoch 0 | Batch 1400/1563 | Loss: 3.2506
Epoch 0 | Batch 1450/1563 | Loss: 3.1546
Epoch 0 | Batch 1500/1563 | Loss: 3.1578
Epoch 0 | Batch 1550/1563 | Loss: 3.3554
Epoch 0 | Time: 138.1s | Train: 3.2974 | Val: 3.7356
New best model saved! Val Loss: 3.7356
Epoch 1 | Batch 0/1563 | Loss: 3.0449
Epoch 1 | Batch 50/1563 | Loss: 3.1116
Epoch 1 | Batch 100/1563 | Loss: 3.2465
Epoch 1 | Batch 150/1563 | Loss: 3.0998
Epoch 1 | Batch 200/1563 | Loss: 3.0975
Epoch 1 | Batch 250/1563 | Loss: 3.0302
Epoch 1 | Batch 300/1563 | Loss: 3.0395
Epoch 1 | Batch 350/1563 | Loss: 3.0812
Epoch 1 | Batch 400/1563 | Loss: 3.1370
Epoch 1 | Batch 450/1563 | Loss: 3.0395
Epoch 1 | Batch 500/1563 | Loss: 3.1562
Epoch 1 | Batch 550/1563 | Loss: 3.1247
Epoch 1 | Batch 600/1563 | Loss: 3.1991
Epoch 1 | Batch 650/1563 | Loss: 3.1580
Epoch 1 | Batch 700/1563 | Loss: 3.1377
Epoch 1 | Batch 750/1563 | Loss: 3.0656
Epoch 1 | Batch 800/1563 | Loss: 3.1628
Epoch 1 | Batch 850/1563 | Loss: 3.0987
Epoch 1 | Batch 900/1563 | Loss: 3.1918
Epoch 1 | Batch 950/1563 | Loss: 3.1478
Epoch 1 | Batch 1000/1563 | Loss: 3.1955
Epoch 1 | Batch 1050/1563 | Loss: 3.0394
Epoch 1 | Batch 1100/1563 | Loss: 3.1439
Epoch 1 | Batch 1150/1563 | Loss: 3.1708
Epoch 1 | Batch 1200/1563 | Loss: 3.1268
Epoch 1 | Batch 1250/1563 | Loss: 3.0425
Epoch 1 | Batch 1300/1563 | Loss: 3.1397
Epoch 1 | Batch 1350/1563 | Loss: 3.0191
Epoch 1 | Batch 1400/1563 | Loss: 3.0370
Epoch 1 | Batch 1450/1563 | Loss: 3.2300
Epoch 1 | Batch 1500/1563 | Loss: 3.0597
Epoch 1 | Batch 1550/1563 | Loss: 3.0430
Epoch 1 | Time: 138.1s | Train: 3.1314 | Val: 3.7460
Epoch 2 | Batch 0/1563 | Loss: 3.0703
Epoch 2 | Batch 50/1563 | Loss: 2.9178
Epoch 2 | Batch 100/1563 | Loss: 3.0748
Epoch 2 | Batch 150/1563 | Loss: 2.9745
Epoch 2 | Batch 200/1563 | Loss: 2.9562
Epoch 2 | Batch 250/1563 | Loss: 3.0512
Epoch 2 | Batch 300/1563 | Loss: 3.0186
Epoch 2 | Batch 350/1563 | Loss: 3.0589
Epoch 2 | Batch 400/1563 | Loss: 3.1146
Epoch 2 | Batch 450/1563 | Loss: 3.0345
Epoch 2 | Batch 500/1563 | Loss: 3.0640
Epoch 2 | Batch 550/1563 | Loss: 3.0902
Epoch 2 | Batch 600/1563 | Loss: 2.9973
Epoch 2 | Batch 650/1563 | Loss: 3.0473
Epoch 2 | Batch 700/1563 | Loss: 2.9746
Epoch 2 | Batch 750/1563 | Loss: 3.0346
Epoch 2 | Batch 800/1563 | Loss: 3.0237
Epoch 2 | Batch 850/1563 | Loss: 3.1530
Epoch 2 | Batch 900/1563 | Loss: 2.9957
Epoch 2 | Batch 950/1563 | Loss: 3.0831
Epoch 2 | Batch 1000/1563 | Loss: 3.0739
Epoch 2 | Batch 1050/1563 | Loss: 3.0553
Epoch 2 | Batch 1100/1563 | Loss: 2.9832
Epoch 2 | Batch 1150/1563 | Loss: 2.8870
Epoch 2 | Batch 1200/1563 | Loss: 2.9067
Epoch 2 | Batch 1250/1563 | Loss: 2.7967
Epoch 2 | Batch 1300/1563 | Loss: 3.0698
Epoch 2 | Batch 1350/1563 | Loss: 2.9261
Epoch 2 | Batch 1400/1563 | Loss: 3.0510
Epoch 2 | Batch 1450/1563 | Loss: 3.1282
Epoch 2 | Batch 1500/1563 | Loss: 2.9391
Epoch 2 | Batch 1550/1563 | Loss: 3.1982
Epoch 2 | Time: 138.2s | Train: 3.0329 | Val: 3.7729
Epoch 3 | Batch 0/1563 | Loss: 2.9959
Epoch 3 | Batch 50/1563 | Loss: 2.9671
Epoch 3 | Batch 100/1563 | Loss: 2.9794
Epoch 3 | Batch 150/1563 | Loss: 2.9932
Epoch 3 | Batch 200/1563 | Loss: 2.9060
Epoch 3 | Batch 250/1563 | Loss: 2.9595
Epoch 3 | Batch 300/1563 | Loss: 2.9846
Epoch 3 | Batch 350/1563 | Loss: 2.9517
Epoch 3 | Batch 400/1563 | Loss: 3.0142
Epoch 3 | Batch 450/1563 | Loss: 2.9566
Epoch 3 | Batch 500/1563 | Loss: 3.0151
Epoch 3 | Batch 550/1563 | Loss: 3.0665
Epoch 3 | Batch 600/1563 | Loss: 3.0299
Epoch 3 | Batch 650/1563 | Loss: 2.9547
Epoch 3 | Batch 700/1563 | Loss: 2.8746
Epoch 3 | Batch 750/1563 | Loss: 3.0479
Epoch 3 | Batch 800/1563 | Loss: 2.8605
Epoch 3 | Batch 850/1563 | Loss: 2.9281
Epoch 3 | Batch 900/1563 | Loss: 2.9480
Epoch 3 | Batch 950/1563 | Loss: 2.9054
Epoch 3 | Batch 1000/1563 | Loss: 2.9224
Epoch 3 | Batch 1050/1563 | Loss: 2.9207
Epoch 3 | Batch 1100/1563 | Loss: 2.9028
Epoch 3 | Batch 1150/1563 | Loss: 2.9063
Epoch 3 | Batch 1200/1563 | Loss: 2.8731
Epoch 3 | Batch 1250/1563 | Loss: 2.9806
Epoch 3 | Batch 1300/1563 | Loss: 2.9239
Epoch 3 | Batch 1350/1563 | Loss: 2.9765
Epoch 3 | Batch 1400/1563 | Loss: 2.8336
Epoch 3 | Batch 1450/1563 | Loss: 3.0464
Epoch 3 | Batch 1500/1563 | Loss: 2.9295
Epoch 3 | Batch 1550/1563 | Loss: 2.9169
Epoch 3 | Time: 137.5s | Train: 2.9518 | Val: 3.7966
Epoch 4 | Batch 0/1563 | Loss: 2.8527
Epoch 4 | Batch 50/1563 | Loss: 2.6374
Epoch 4 | Batch 100/1563 | Loss: 2.9429
Epoch 4 | Batch 150/1563 | Loss: 2.8257
Epoch 4 | Batch 200/1563 | Loss: 2.9029
Epoch 4 | Batch 250/1563 | Loss: 2.8408
Epoch 4 | Batch 300/1563 | Loss: 2.7735
Epoch 4 | Batch 350/1563 | Loss: 2.7606
Epoch 4 | Batch 400/1563 | Loss: 2.8617
Epoch 4 | Batch 450/1563 | Loss: 2.8462
Epoch 4 | Batch 500/1563 | Loss: 2.8459
Epoch 4 | Batch 550/1563 | Loss: 2.7856
Epoch 4 | Batch 600/1563 | Loss: 2.9245
Epoch 4 | Batch 650/1563 | Loss: 2.8917
Epoch 4 | Batch 700/1563 | Loss: 2.7366
Epoch 4 | Batch 750/1563 | Loss: 2.9429
Epoch 4 | Batch 800/1563 | Loss: 2.8775
Epoch 4 | Batch 850/1563 | Loss: 2.9560
Epoch 4 | Batch 900/1563 | Loss: 2.8276
Epoch 4 | Batch 950/1563 | Loss: 2.9212
Epoch 4 | Batch 1000/1563 | Loss: 2.9202
Epoch 4 | Batch 1050/1563 | Loss: 2.8434
Epoch 4 | Batch 1100/1563 | Loss: 2.8993
Epoch 4 | Batch 1150/1563 | Loss: 2.9936
Epoch 4 | Batch 1200/1563 | Loss: 2.9701
Epoch 4 | Batch 1250/1563 | Loss: 2.8583
Epoch 4 | Batch 1300/1563 | Loss: 2.8682
Epoch 4 | Batch 1350/1563 | Loss: 2.8586
Epoch 4 | Batch 1400/1563 | Loss: 2.9595
Epoch 4 | Batch 1450/1563 | Loss: 2.7764
Epoch 4 | Batch 1500/1563 | Loss: 2.9008
Epoch 4 | Batch 1550/1563 | Loss: 2.9563
Epoch 4 | Time: 137.6s | Train: 2.8818 | Val: 3.8276
Epoch 5 | Batch 0/1563 | Loss: 2.7729
Epoch 5 | Batch 50/1563 | Loss: 2.8130
Epoch 5 | Batch 100/1563 | Loss: 2.7779
Epoch 5 | Batch 150/1563 | Loss: 2.7832
Epoch 5 | Batch 200/1563 | Loss: 2.8594
Epoch 5 | Batch 250/1563 | Loss: 2.8430
Epoch 5 | Batch 300/1563 | Loss: 2.8657
Epoch 5 | Batch 350/1563 | Loss: 2.8523
Epoch 5 | Batch 400/1563 | Loss: 2.8170
Epoch 5 | Batch 450/1563 | Loss: 2.8204
Epoch 5 | Batch 500/1563 | Loss: 2.7605
Epoch 5 | Batch 550/1563 | Loss: 2.8976
Epoch 5 | Batch 600/1563 | Loss: 2.8677
Epoch 5 | Batch 650/1563 | Loss: 2.7624
Epoch 5 | Batch 700/1563 | Loss: 2.8149
Epoch 5 | Batch 750/1563 | Loss: 2.8413
Epoch 5 | Batch 800/1563 | Loss: 2.8631
Epoch 5 | Batch 850/1563 | Loss: 2.8701
Epoch 5 | Batch 900/1563 | Loss: 2.9254
Epoch 5 | Batch 950/1563 | Loss: 2.8145
Epoch 5 | Batch 1000/1563 | Loss: 3.0216
Epoch 5 | Batch 1050/1563 | Loss: 2.8563
Epoch 5 | Batch 1100/1563 | Loss: 2.7873
Epoch 5 | Batch 1150/1563 | Loss: 2.8417
Epoch 5 | Batch 1200/1563 | Loss: 2.7809
Epoch 5 | Batch 1250/1563 | Loss: 2.8823
Epoch 5 | Batch 1300/1563 | Loss: 2.7367
Epoch 5 | Batch 1350/1563 | Loss: 2.8231
Epoch 5 | Batch 1400/1563 | Loss: 2.9091
Epoch 5 | Batch 1450/1563 | Loss: 2.9202
Epoch 5 | Batch 1500/1563 | Loss: 2.8642
Epoch 5 | Batch 1550/1563 | Loss: 2.9747
Epoch 5 | Time: 137.7s | Train: 2.8176 | Val: 3.8625
Epoch 6 | Batch 0/1563 | Loss: 2.6566
Epoch 6 | Batch 50/1563 | Loss: 2.7953
Epoch 6 | Batch 100/1563 | Loss: 2.7271
Epoch 6 | Batch 150/1563 | Loss: 2.6784
Epoch 6 | Batch 200/1563 | Loss: 2.8720
Epoch 6 | Batch 250/1563 | Loss: 2.6931
Epoch 6 | Batch 300/1563 | Loss: 2.6771
Epoch 6 | Batch 350/1563 | Loss: 2.7958
Epoch 6 | Batch 400/1563 | Loss: 2.6888
Epoch 6 | Batch 450/1563 | Loss: 2.7379
Epoch 6 | Batch 500/1563 | Loss: 2.6867
Epoch 6 | Batch 550/1563 | Loss: 2.8306
Epoch 6 | Batch 600/1563 | Loss: 2.7144
Epoch 6 | Batch 650/1563 | Loss: 2.7277
Epoch 6 | Batch 700/1563 | Loss: 2.6547
Epoch 6 | Batch 750/1563 | Loss: 2.6914
Epoch 6 | Batch 800/1563 | Loss: 2.6383
Epoch 6 | Batch 850/1563 | Loss: 2.8860
Epoch 6 | Batch 900/1563 | Loss: 2.6986
Epoch 6 | Batch 950/1563 | Loss: 2.7657
Epoch 6 | Batch 1000/1563 | Loss: 2.8925
Epoch 6 | Batch 1050/1563 | Loss: 2.7847
Epoch 6 | Batch 1100/1563 | Loss: 2.8743
Epoch 6 | Batch 1150/1563 | Loss: 2.7344
Epoch 6 | Batch 1200/1563 | Loss: 2.7826
Epoch 6 | Batch 1250/1563 | Loss: 2.8179
Epoch 6 | Batch 1300/1563 | Loss: 2.7588
Epoch 6 | Batch 1350/1563 | Loss: 2.9109
Epoch 6 | Batch 1400/1563 | Loss: 2.8638
Epoch 6 | Batch 1450/1563 | Loss: 2.7232
Epoch 6 | Batch 1500/1563 | Loss: 2.6660
Epoch 6 | Batch 1550/1563 | Loss: 2.8987
Epoch 6 | Time: 137.4s | Train: 2.7589 | Val: 3.9040
Epoch 7 | Batch 0/1563 | Loss: 2.6193
Epoch 7 | Batch 50/1563 | Loss: 2.7138
Epoch 7 | Batch 100/1563 | Loss: 2.7363
Epoch 7 | Batch 150/1563 | Loss: 2.5700
Epoch 7 | Batch 200/1563 | Loss: 2.7009
Epoch 7 | Batch 250/1563 | Loss: 2.7205
Epoch 7 | Batch 300/1563 | Loss: 2.9094
Epoch 7 | Batch 350/1563 | Loss: 2.6223
Epoch 7 | Batch 400/1563 | Loss: 2.5763
Epoch 7 | Batch 450/1563 | Loss: 2.8010
Epoch 7 | Batch 500/1563 | Loss: 2.8416
Epoch 7 | Batch 550/1563 | Loss: 2.6690
Epoch 7 | Batch 600/1563 | Loss: 2.6887
Epoch 7 | Batch 650/1563 | Loss: 2.7002
Epoch 7 | Batch 700/1563 | Loss: 2.7153
Epoch 7 | Batch 750/1563 | Loss: 2.6072
Epoch 7 | Batch 800/1563 | Loss: 2.7893
Epoch 7 | Batch 850/1563 | Loss: 2.8669
Epoch 7 | Batch 900/1563 | Loss: 2.7740
Epoch 7 | Batch 950/1563 | Loss: 2.7904
Epoch 7 | Batch 1000/1563 | Loss: 2.7536
Epoch 7 | Batch 1050/1563 | Loss: 2.6203
Epoch 7 | Batch 1100/1563 | Loss: 2.7932
Epoch 7 | Batch 1150/1563 | Loss: 2.8599
Epoch 7 | Batch 1200/1563 | Loss: 2.8082
Epoch 7 | Batch 1250/1563 | Loss: 2.7520
Epoch 7 | Batch 1300/1563 | Loss: 2.6932
Epoch 7 | Batch 1350/1563 | Loss: 2.7470
Epoch 7 | Batch 1400/1563 | Loss: 2.6230
Epoch 7 | Batch 1450/1563 | Loss: 2.7501
Epoch 7 | Batch 1500/1563 | Loss: 2.7363
Epoch 7 | Batch 1550/1563 | Loss: 2.6756
Epoch 7 | Time: 140.7s | Train: 2.7041 | Val: 3.9267
Epoch 8 | Batch 0/1563 | Loss: 2.5065
Epoch 8 | Batch 50/1563 | Loss: 2.6364
Epoch 8 | Batch 100/1563 | Loss: 2.6085
Epoch 8 | Batch 150/1563 | Loss: 2.5248
Epoch 8 | Batch 200/1563 | Loss: 2.5483
Epoch 8 | Batch 250/1563 | Loss: 2.5756
Epoch 8 | Batch 300/1563 | Loss: 2.6670
Epoch 8 | Batch 350/1563 | Loss: 2.5758
Epoch 8 | Batch 400/1563 | Loss: 2.6120
Epoch 8 | Batch 450/1563 | Loss: 2.5033
Epoch 8 | Batch 500/1563 | Loss: 2.5599
Epoch 8 | Batch 550/1563 | Loss: 2.6734
Epoch 8 | Batch 600/1563 | Loss: 2.6270
Epoch 8 | Batch 650/1563 | Loss: 2.7193
Epoch 8 | Batch 700/1563 | Loss: 2.7916
Epoch 8 | Batch 750/1563 | Loss: 2.6620
Epoch 8 | Batch 800/1563 | Loss: 2.5714
Epoch 8 | Batch 850/1563 | Loss: 2.5528
Epoch 8 | Batch 900/1563 | Loss: 2.7228
Epoch 8 | Batch 950/1563 | Loss: 2.7018
Epoch 8 | Batch 1000/1563 | Loss: 2.5988
Epoch 8 | Batch 1050/1563 | Loss: 2.8699
Epoch 8 | Batch 1100/1563 | Loss: 2.6505
Epoch 8 | Batch 1150/1563 | Loss: 2.5750
Epoch 8 | Batch 1200/1563 | Loss: 2.8180
Epoch 8 | Batch 1250/1563 | Loss: 2.7196
Epoch 8 | Batch 1300/1563 | Loss: 2.6062
Epoch 8 | Batch 1350/1563 | Loss: 2.6812
Epoch 8 | Batch 1400/1563 | Loss: 2.6675
Epoch 8 | Batch 1450/1563 | Loss: 2.7487
Epoch 8 | Batch 1500/1563 | Loss: 2.9828
Epoch 8 | Batch 1550/1563 | Loss: 2.7138
Epoch 8 | Time: 139.5s | Train: 2.6521 | Val: 3.9753
Epoch 9 | Batch 0/1563 | Loss: 2.5806
Epoch 9 | Batch 50/1563 | Loss: 2.5422
Epoch 9 | Batch 100/1563 | Loss: 2.4910
Epoch 9 | Batch 150/1563 | Loss: 2.6354
Epoch 9 | Batch 200/1563 | Loss: 2.4774
Epoch 9 | Batch 250/1563 | Loss: 2.6893
Epoch 9 | Batch 300/1563 | Loss: 2.5926
Epoch 9 | Batch 350/1563 | Loss: 2.4606
Epoch 9 | Batch 400/1563 | Loss: 2.6058
Epoch 9 | Batch 450/1563 | Loss: 2.6746
Epoch 9 | Batch 500/1563 | Loss: 2.6584
Epoch 9 | Batch 550/1563 | Loss: 2.4388
Epoch 9 | Batch 600/1563 | Loss: 2.5467
Epoch 9 | Batch 650/1563 | Loss: 2.5621
Epoch 9 | Batch 700/1563 | Loss: 2.5678
Epoch 9 | Batch 750/1563 | Loss: 2.6164
Epoch 9 | Batch 800/1563 | Loss: 2.6330
Epoch 9 | Batch 850/1563 | Loss: 2.6973
Epoch 9 | Batch 900/1563 | Loss: 2.6444
Epoch 9 | Batch 950/1563 | Loss: 2.7211
Epoch 9 | Batch 1000/1563 | Loss: 2.5846
Epoch 9 | Batch 1050/1563 | Loss: 2.6501
Epoch 9 | Batch 1100/1563 | Loss: 2.5382
Epoch 9 | Batch 1150/1563 | Loss: 2.6616
Epoch 9 | Batch 1200/1563 | Loss: 2.5996
Epoch 9 | Batch 1250/1563 | Loss: 2.5961
Epoch 9 | Batch 1300/1563 | Loss: 2.5443
Epoch 9 | Batch 1350/1563 | Loss: 2.7471
Epoch 9 | Batch 1400/1563 | Loss: 2.6341
Epoch 9 | Batch 1450/1563 | Loss: 2.6060
Epoch 9 | Batch 1500/1563 | Loss: 2.7369
Epoch 9 | Batch 1550/1563 | Loss: 2.5583
Epoch 9 | Time: 137.3s | Train: 2.6026 | Val: 4.0136

Done! Best Val Loss: 3.7356
