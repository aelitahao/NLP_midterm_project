You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
Initializing t5-large on cuda...
Running full-parameter fine-tuning.
Loading data from data/raw/train_100k.jsonl...
Loaded 100000 samples.
Loading data from data/raw/valid.jsonl...
Loaded 500 samples.

Start Training: 100k | Effective Batch Size: 32 | AMP: bf16
/usr/local/lib/python3.10/dist-packages/apex/_autocast_utils.py:26: FutureWarning: `torch.cuda.amp.autocast_mode._cast(value, dtype)` is deprecated. Please use `torch.amp.autocast_mode._cast(value, 'cuda', dtype)` instead.
  return torch.cuda.amp.autocast_mode._cast(args, torch.get_autocast_gpu_dtype())
/usr/local/lib/python3.10/dist-packages/apex/normalization/fused_layer_norm.py:214: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=False):
Epoch 0 | Batch 0/3125 | Loss: 3.6483
Epoch 0 | Batch 50/3125 | Loss: 3.0492
Epoch 0 | Batch 100/3125 | Loss: 3.0725
Epoch 0 | Batch 150/3125 | Loss: 3.1700
Epoch 0 | Batch 200/3125 | Loss: 3.1050
Epoch 0 | Batch 250/3125 | Loss: 3.2398
Epoch 0 | Batch 300/3125 | Loss: 3.0220
Epoch 0 | Batch 350/3125 | Loss: 3.0798
Epoch 0 | Batch 400/3125 | Loss: 3.0909
Epoch 0 | Batch 450/3125 | Loss: 3.2056
Epoch 0 | Batch 500/3125 | Loss: 3.0011
Epoch 0 | Batch 550/3125 | Loss: 2.9938
Epoch 0 | Batch 600/3125 | Loss: 3.1785
Epoch 0 | Batch 650/3125 | Loss: 3.1311
Epoch 0 | Batch 700/3125 | Loss: 3.0822
Epoch 0 | Batch 750/3125 | Loss: 2.8136
Epoch 0 | Batch 800/3125 | Loss: 2.9438
Epoch 0 | Batch 850/3125 | Loss: 3.1194
Epoch 0 | Batch 900/3125 | Loss: 3.0215
Epoch 0 | Batch 950/3125 | Loss: 3.0079
Epoch 0 | Batch 1000/3125 | Loss: 3.0124
Epoch 0 | Batch 1050/3125 | Loss: 3.1491
Epoch 0 | Batch 1100/3125 | Loss: 3.0141
Epoch 0 | Batch 1150/3125 | Loss: 3.0511
Epoch 0 | Batch 1200/3125 | Loss: 3.1276
Epoch 0 | Batch 1250/3125 | Loss: 2.9781
Epoch 0 | Batch 1300/3125 | Loss: 2.9408
Epoch 0 | Batch 1350/3125 | Loss: 3.1632
Epoch 0 | Batch 1400/3125 | Loss: 2.9324
Epoch 0 | Batch 1450/3125 | Loss: 3.0950
Epoch 0 | Batch 1500/3125 | Loss: 3.0604
Epoch 0 | Batch 1550/3125 | Loss: 3.1944
Epoch 0 | Batch 1600/3125 | Loss: 2.9684
Epoch 0 | Batch 1650/3125 | Loss: 3.0600
Epoch 0 | Batch 1700/3125 | Loss: 2.9510
Epoch 0 | Batch 1750/3125 | Loss: 3.1921
Epoch 0 | Batch 1800/3125 | Loss: 2.9795
Epoch 0 | Batch 1850/3125 | Loss: 3.1165
Epoch 0 | Batch 1900/3125 | Loss: 3.1101
Epoch 0 | Batch 1950/3125 | Loss: 3.0484
Epoch 0 | Batch 2000/3125 | Loss: 3.1158
Epoch 0 | Batch 2050/3125 | Loss: 3.1158
Epoch 0 | Batch 2100/3125 | Loss: 3.0081
Epoch 0 | Batch 2150/3125 | Loss: 3.1469
Epoch 0 | Batch 2200/3125 | Loss: 3.0289
Epoch 0 | Batch 2250/3125 | Loss: 3.0252
Epoch 0 | Batch 2300/3125 | Loss: 3.1782
Epoch 0 | Batch 2350/3125 | Loss: 3.1054
Epoch 0 | Batch 2400/3125 | Loss: 3.1544
Epoch 0 | Batch 2450/3125 | Loss: 3.0601
Epoch 0 | Batch 2500/3125 | Loss: 2.9721
Epoch 0 | Batch 2550/3125 | Loss: 2.9745
Epoch 0 | Batch 2600/3125 | Loss: 2.9953
Epoch 0 | Batch 2650/3125 | Loss: 2.9120
Epoch 0 | Batch 2700/3125 | Loss: 3.0441
Epoch 0 | Batch 2750/3125 | Loss: 2.9419
Epoch 0 | Batch 2800/3125 | Loss: 2.8753
Epoch 0 | Batch 2850/3125 | Loss: 2.9433
Epoch 0 | Batch 2900/3125 | Loss: 2.8607
Epoch 0 | Batch 2950/3125 | Loss: 3.0522
Epoch 0 | Batch 3000/3125 | Loss: 3.0860
Epoch 0 | Batch 3050/3125 | Loss: 3.1049
Epoch 0 | Batch 3100/3125 | Loss: 3.1404
Epoch 0 | Time: 472.0s | Train: 3.0547 | Val: 3.6344
New best model saved! Val Loss: 3.6344
Epoch 1 | Batch 0/3125 | Loss: 2.8404
Epoch 1 | Batch 50/3125 | Loss: 2.8282
Epoch 1 | Batch 100/3125 | Loss: 2.7990
Epoch 1 | Batch 150/3125 | Loss: 2.7754
Epoch 1 | Batch 200/3125 | Loss: 2.7741
Epoch 1 | Batch 250/3125 | Loss: 2.9180
Epoch 1 | Batch 300/3125 | Loss: 2.6967
Epoch 1 | Batch 350/3125 | Loss: 2.4649
Epoch 1 | Batch 400/3125 | Loss: 2.8157
Epoch 1 | Batch 450/3125 | Loss: 2.7039
Epoch 1 | Batch 500/3125 | Loss: 2.6953
Epoch 1 | Batch 550/3125 | Loss: 2.8369
Epoch 1 | Batch 600/3125 | Loss: 2.6624
Epoch 1 | Batch 650/3125 | Loss: 2.7745
Epoch 1 | Batch 700/3125 | Loss: 2.8432
Epoch 1 | Batch 750/3125 | Loss: 2.8197
Epoch 1 | Batch 800/3125 | Loss: 2.8214
Epoch 1 | Batch 850/3125 | Loss: 2.6332
Epoch 1 | Batch 900/3125 | Loss: 2.8342
Epoch 1 | Batch 950/3125 | Loss: 2.9324
Epoch 1 | Batch 1000/3125 | Loss: 2.8545
Epoch 1 | Batch 1050/3125 | Loss: 2.8631
Epoch 1 | Batch 1100/3125 | Loss: 2.7553
Epoch 1 | Batch 1150/3125 | Loss: 2.8793
Epoch 1 | Batch 1200/3125 | Loss: 2.7598
Epoch 1 | Batch 1250/3125 | Loss: 2.9337
Epoch 1 | Batch 1300/3125 | Loss: 2.8076
Epoch 1 | Batch 1350/3125 | Loss: 2.7738
Epoch 1 | Batch 1400/3125 | Loss: 2.8434
Epoch 1 | Batch 1450/3125 | Loss: 2.7617
Epoch 1 | Batch 1500/3125 | Loss: 2.9069
Epoch 1 | Batch 1550/3125 | Loss: 2.9274
Epoch 1 | Batch 1600/3125 | Loss: 2.7375
Epoch 1 | Batch 1650/3125 | Loss: 2.8867
Epoch 1 | Batch 1700/3125 | Loss: 2.6864
Epoch 1 | Batch 1750/3125 | Loss: 2.7864
Epoch 1 | Batch 1800/3125 | Loss: 2.7613
Epoch 1 | Batch 1850/3125 | Loss: 2.9332
Epoch 1 | Batch 1900/3125 | Loss: 2.9373
Epoch 1 | Batch 1950/3125 | Loss: 2.9581
Epoch 1 | Batch 2000/3125 | Loss: 2.7021
Epoch 1 | Batch 2050/3125 | Loss: 3.0297
Epoch 1 | Batch 2100/3125 | Loss: 2.8848
Epoch 1 | Batch 2150/3125 | Loss: 2.9104
Epoch 1 | Batch 2200/3125 | Loss: 2.9734
Epoch 1 | Batch 2250/3125 | Loss: 2.7645
Epoch 1 | Batch 2300/3125 | Loss: 2.9576
Epoch 1 | Batch 2350/3125 | Loss: 2.9004
Epoch 1 | Batch 2400/3125 | Loss: 2.9704
Epoch 1 | Batch 2450/3125 | Loss: 2.7684
Epoch 1 | Batch 2500/3125 | Loss: 2.7851
Epoch 1 | Batch 2550/3125 | Loss: 2.8962
Epoch 1 | Batch 2600/3125 | Loss: 2.8650
Epoch 1 | Batch 2650/3125 | Loss: 2.8331
Epoch 1 | Batch 2700/3125 | Loss: 2.8806
Epoch 1 | Batch 2750/3125 | Loss: 2.9000
Epoch 1 | Batch 2800/3125 | Loss: 2.8872
Epoch 1 | Batch 2850/3125 | Loss: 2.7771
Epoch 1 | Batch 2900/3125 | Loss: 2.8400
Epoch 1 | Batch 2950/3125 | Loss: 2.8164
Epoch 1 | Batch 3000/3125 | Loss: 2.6797
Epoch 1 | Batch 3050/3125 | Loss: 2.6285
Epoch 1 | Batch 3100/3125 | Loss: 2.9339
Epoch 1 | Time: 481.2s | Train: 2.8208 | Val: 3.6752
Epoch 2 | Batch 0/3125 | Loss: 2.3599
Epoch 2 | Batch 50/3125 | Loss: 2.5392
Epoch 2 | Batch 100/3125 | Loss: 2.6876
Epoch 2 | Batch 150/3125 | Loss: 2.6242
Epoch 2 | Batch 200/3125 | Loss: 2.6420
Epoch 2 | Batch 250/3125 | Loss: 2.5446
Epoch 2 | Batch 300/3125 | Loss: 2.4666
Epoch 2 | Batch 350/3125 | Loss: 2.5759
Epoch 2 | Batch 400/3125 | Loss: 2.6011
Epoch 2 | Batch 450/3125 | Loss: 2.4337
Epoch 2 | Batch 500/3125 | Loss: 2.6371
Epoch 2 | Batch 550/3125 | Loss: 2.7568
Epoch 2 | Batch 600/3125 | Loss: 2.4192
Epoch 2 | Batch 650/3125 | Loss: 2.5286
Epoch 2 | Batch 700/3125 | Loss: 2.5200
Epoch 2 | Batch 750/3125 | Loss: 2.5861
Epoch 2 | Batch 800/3125 | Loss: 2.7004
Epoch 2 | Batch 850/3125 | Loss: 2.7969
Epoch 2 | Batch 900/3125 | Loss: 2.5410
Epoch 2 | Batch 950/3125 | Loss: 2.5336
Epoch 2 | Batch 1000/3125 | Loss: 2.5238
Epoch 2 | Batch 1050/3125 | Loss: 2.5703
Epoch 2 | Batch 1100/3125 | Loss: 2.6966
Epoch 2 | Batch 1150/3125 | Loss: 2.7458
Epoch 2 | Batch 1200/3125 | Loss: 2.7356
Epoch 2 | Batch 1250/3125 | Loss: 2.6132
Epoch 2 | Batch 1300/3125 | Loss: 2.7236
Epoch 2 | Batch 1350/3125 | Loss: 2.7501
Epoch 2 | Batch 1400/3125 | Loss: 2.5305
Epoch 2 | Batch 1450/3125 | Loss: 2.6255
Epoch 2 | Batch 1500/3125 | Loss: 2.6793
Epoch 2 | Batch 1550/3125 | Loss: 2.7910
Epoch 2 | Batch 1600/3125 | Loss: 2.7016
Epoch 2 | Batch 1650/3125 | Loss: 2.7548
Epoch 2 | Batch 1700/3125 | Loss: 2.7978
Epoch 2 | Batch 1750/3125 | Loss: 2.5788
Epoch 2 | Batch 1800/3125 | Loss: 2.6737
Epoch 2 | Batch 1850/3125 | Loss: 2.7762
Epoch 2 | Batch 1900/3125 | Loss: 2.6471
Epoch 2 | Batch 1950/3125 | Loss: 2.7416
Epoch 2 | Batch 2000/3125 | Loss: 2.7769
Epoch 2 | Batch 2050/3125 | Loss: 2.7186
Epoch 2 | Batch 2100/3125 | Loss: 2.6961
Epoch 2 | Batch 2150/3125 | Loss: 2.6333
Epoch 2 | Batch 2200/3125 | Loss: 2.7682
Epoch 2 | Batch 2250/3125 | Loss: 2.6027
Epoch 2 | Batch 2300/3125 | Loss: 2.8170
Epoch 2 | Batch 2350/3125 | Loss: 2.5496
Epoch 2 | Batch 2400/3125 | Loss: 2.4662
Epoch 2 | Batch 2450/3125 | Loss: 2.8037
Epoch 2 | Batch 2500/3125 | Loss: 2.7458
Epoch 2 | Batch 2550/3125 | Loss: 2.6593
Epoch 2 | Batch 2600/3125 | Loss: 2.8408
Epoch 2 | Batch 2650/3125 | Loss: 2.6571
Epoch 2 | Batch 2700/3125 | Loss: 2.5495
Epoch 2 | Batch 2750/3125 | Loss: 2.7621
Epoch 2 | Batch 2800/3125 | Loss: 2.8638
Epoch 2 | Batch 2850/3125 | Loss: 2.7333
Epoch 2 | Batch 2900/3125 | Loss: 2.6164
Epoch 2 | Batch 2950/3125 | Loss: 2.7273
Epoch 2 | Batch 3000/3125 | Loss: 2.7403
Epoch 2 | Batch 3050/3125 | Loss: 2.7505
Epoch 2 | Batch 3100/3125 | Loss: 2.7990
Epoch 2 | Time: 474.8s | Train: 2.6536 | Val: 3.7191
Epoch 3 | Batch 0/3125 | Loss: 2.2787
Epoch 3 | Batch 50/3125 | Loss: 2.4538
Epoch 3 | Batch 100/3125 | Loss: 2.5189
Epoch 3 | Batch 150/3125 | Loss: 2.3784
Epoch 3 | Batch 200/3125 | Loss: 2.5522
Epoch 3 | Batch 250/3125 | Loss: 2.4731
Epoch 3 | Batch 300/3125 | Loss: 2.4214
Epoch 3 | Batch 350/3125 | Loss: 2.4608
Epoch 3 | Batch 400/3125 | Loss: 2.3860
Epoch 3 | Batch 450/3125 | Loss: 2.4424
Epoch 3 | Batch 500/3125 | Loss: 2.5812
Epoch 3 | Batch 550/3125 | Loss: 2.6045
Epoch 3 | Batch 600/3125 | Loss: 2.4538
Epoch 3 | Batch 650/3125 | Loss: 2.4708
Epoch 3 | Batch 700/3125 | Loss: 2.5721
Epoch 3 | Batch 750/3125 | Loss: 2.5047
Epoch 3 | Batch 800/3125 | Loss: 2.4394
Epoch 3 | Batch 850/3125 | Loss: 2.5060
Epoch 3 | Batch 900/3125 | Loss: 2.5037
Epoch 3 | Batch 950/3125 | Loss: 2.4917
Epoch 3 | Batch 1000/3125 | Loss: 2.5018
Epoch 3 | Batch 1050/3125 | Loss: 2.5357
Epoch 3 | Batch 1100/3125 | Loss: 2.4968
Epoch 3 | Batch 1150/3125 | Loss: 2.3676
Epoch 3 | Batch 1200/3125 | Loss: 2.4160
Epoch 3 | Batch 1250/3125 | Loss: 2.5308
Epoch 3 | Batch 1300/3125 | Loss: 2.4504
Epoch 3 | Batch 1350/3125 | Loss: 2.4095
Epoch 3 | Batch 1400/3125 | Loss: 2.5197
Epoch 3 | Batch 1450/3125 | Loss: 2.5113
Epoch 3 | Batch 1500/3125 | Loss: 2.4789
Epoch 3 | Batch 1550/3125 | Loss: 2.6003
Epoch 3 | Batch 1600/3125 | Loss: 2.5299
Epoch 3 | Batch 1650/3125 | Loss: 2.5773
Epoch 3 | Batch 1700/3125 | Loss: 2.6569
Epoch 3 | Batch 1750/3125 | Loss: 2.5047
Epoch 3 | Batch 1800/3125 | Loss: 2.6142
Epoch 3 | Batch 1850/3125 | Loss: 2.3548
Epoch 3 | Batch 1900/3125 | Loss: 2.6374
Epoch 3 | Batch 1950/3125 | Loss: 2.4657
Epoch 3 | Batch 2000/3125 | Loss: 2.5216
Epoch 3 | Batch 2050/3125 | Loss: 2.4376
Epoch 3 | Batch 2100/3125 | Loss: 2.6386
Epoch 3 | Batch 2150/3125 | Loss: 2.3741
Epoch 3 | Batch 2200/3125 | Loss: 2.4822
Epoch 3 | Batch 2250/3125 | Loss: 2.5689
Epoch 3 | Batch 2300/3125 | Loss: 2.4326
Epoch 3 | Batch 2350/3125 | Loss: 2.5952
Epoch 3 | Batch 2400/3125 | Loss: 2.5128
Epoch 3 | Batch 2450/3125 | Loss: 2.5907
Epoch 3 | Batch 2500/3125 | Loss: 2.6834
Epoch 3 | Batch 2550/3125 | Loss: 2.4889
Epoch 3 | Batch 2600/3125 | Loss: 2.5521
Epoch 3 | Batch 2650/3125 | Loss: 2.4619
Epoch 3 | Batch 2700/3125 | Loss: 2.5420
Epoch 3 | Batch 2750/3125 | Loss: 2.5630
Epoch 3 | Batch 2800/3125 | Loss: 2.6919
Epoch 3 | Batch 2850/3125 | Loss: 2.4750
Epoch 3 | Batch 2900/3125 | Loss: 2.5978
Epoch 3 | Batch 2950/3125 | Loss: 2.4406
Epoch 3 | Batch 3000/3125 | Loss: 2.5466
Epoch 3 | Batch 3050/3125 | Loss: 2.6201
Epoch 3 | Batch 3100/3125 | Loss: 2.5638
Epoch 3 | Time: 475.7s | Train: 2.5026 | Val: 3.8303
Epoch 4 | Batch 0/3125 | Loss: 2.4715
Epoch 4 | Batch 50/3125 | Loss: 2.2140
Epoch 4 | Batch 100/3125 | Loss: 2.4179
Epoch 4 | Batch 150/3125 | Loss: 2.4404
Epoch 4 | Batch 200/3125 | Loss: 2.3341
Epoch 4 | Batch 250/3125 | Loss: 2.4293
Epoch 4 | Batch 300/3125 | Loss: 2.2244
Epoch 4 | Batch 350/3125 | Loss: 2.2408
Epoch 4 | Batch 400/3125 | Loss: 2.3355
Epoch 4 | Batch 450/3125 | Loss: 2.3680
Epoch 4 | Batch 500/3125 | Loss: 2.2886
Epoch 4 | Batch 550/3125 | Loss: 2.3365
Epoch 4 | Batch 600/3125 | Loss: 2.2023
Epoch 4 | Batch 650/3125 | Loss: 2.3475
Epoch 4 | Batch 700/3125 | Loss: 2.4696
Epoch 4 | Batch 750/3125 | Loss: 2.2671
Epoch 4 | Batch 800/3125 | Loss: 2.3080
Epoch 4 | Batch 850/3125 | Loss: 2.4043
Epoch 4 | Batch 900/3125 | Loss: 2.5262
Epoch 4 | Batch 950/3125 | Loss: 2.2777
Epoch 4 | Batch 1000/3125 | Loss: 2.3835
Epoch 4 | Batch 1050/3125 | Loss: 2.2691
Epoch 4 | Batch 1100/3125 | Loss: 2.5151
Epoch 4 | Batch 1150/3125 | Loss: 2.4510
Epoch 4 | Batch 1200/3125 | Loss: 2.2074
Epoch 4 | Batch 1250/3125 | Loss: 2.4061
Epoch 4 | Batch 1300/3125 | Loss: 2.3579
Epoch 4 | Batch 1350/3125 | Loss: 2.5120
Epoch 4 | Batch 1400/3125 | Loss: 2.3422
Epoch 4 | Batch 1450/3125 | Loss: 2.3610
Epoch 4 | Batch 1500/3125 | Loss: 2.3872
Epoch 4 | Batch 1550/3125 | Loss: 2.3156
Epoch 4 | Batch 1600/3125 | Loss: 2.5198
Epoch 4 | Batch 1650/3125 | Loss: 2.2731
Epoch 4 | Batch 1700/3125 | Loss: 2.5787
Epoch 4 | Batch 1750/3125 | Loss: 2.2832
Epoch 4 | Batch 1800/3125 | Loss: 2.3175
Epoch 4 | Batch 1850/3125 | Loss: 2.3519
Epoch 4 | Batch 1900/3125 | Loss: 2.3577
Epoch 4 | Batch 1950/3125 | Loss: 2.1581
Epoch 4 | Batch 2000/3125 | Loss: 2.3683
Epoch 4 | Batch 2050/3125 | Loss: 2.2975
Epoch 4 | Batch 2100/3125 | Loss: 2.3910
Epoch 4 | Batch 2150/3125 | Loss: 2.5023
Epoch 4 | Batch 2200/3125 | Loss: 2.4873
Epoch 4 | Batch 2250/3125 | Loss: 2.5567
Epoch 4 | Batch 2300/3125 | Loss: 2.5218
Epoch 4 | Batch 2350/3125 | Loss: 2.1835
Epoch 4 | Batch 2400/3125 | Loss: 2.4203
Epoch 4 | Batch 2450/3125 | Loss: 2.5376
Epoch 4 | Batch 2500/3125 | Loss: 2.3324
Epoch 4 | Batch 2550/3125 | Loss: 2.7044
Epoch 4 | Batch 2600/3125 | Loss: 2.4261
Epoch 4 | Batch 2650/3125 | Loss: 2.4443
Epoch 4 | Batch 2700/3125 | Loss: 2.5180
Epoch 4 | Batch 2750/3125 | Loss: 2.2741
Epoch 4 | Batch 2800/3125 | Loss: 2.4325
Epoch 4 | Batch 2850/3125 | Loss: 2.1576
Epoch 4 | Batch 2900/3125 | Loss: 2.3973
Epoch 4 | Batch 2950/3125 | Loss: 2.5408
Epoch 4 | Batch 3000/3125 | Loss: 2.4368
Epoch 4 | Batch 3050/3125 | Loss: 2.3783
Epoch 4 | Batch 3100/3125 | Loss: 2.3071
Epoch 4 | Time: 481.6s | Train: 2.3648 | Val: 3.8947
Epoch 5 | Batch 0/3125 | Loss: 2.1260
Epoch 5 | Batch 50/3125 | Loss: 2.2127
Epoch 5 | Batch 100/3125 | Loss: 1.8827
Epoch 5 | Batch 150/3125 | Loss: 2.1167
Epoch 5 | Batch 200/3125 | Loss: 2.0630
Epoch 5 | Batch 250/3125 | Loss: 2.2105
Epoch 5 | Batch 300/3125 | Loss: 2.1521
Epoch 5 | Batch 350/3125 | Loss: 2.1992
Epoch 5 | Batch 400/3125 | Loss: 2.2655
Epoch 5 | Batch 450/3125 | Loss: 2.3354
Epoch 5 | Batch 500/3125 | Loss: 2.3933
Epoch 5 | Batch 550/3125 | Loss: 2.2228
Epoch 5 | Batch 600/3125 | Loss: 2.2588
Epoch 5 | Batch 650/3125 | Loss: 2.2473
Epoch 5 | Batch 700/3125 | Loss: 2.0730
Epoch 5 | Batch 750/3125 | Loss: 2.1591
Epoch 5 | Batch 800/3125 | Loss: 2.3669
Epoch 5 | Batch 850/3125 | Loss: 2.1124
Epoch 5 | Batch 900/3125 | Loss: 2.2468
Epoch 5 | Batch 950/3125 | Loss: 2.2412
Epoch 5 | Batch 1000/3125 | Loss: 2.2434
Epoch 5 | Batch 1050/3125 | Loss: 1.8569
Epoch 5 | Batch 1100/3125 | Loss: 2.2596
Epoch 5 | Batch 1150/3125 | Loss: 2.2224
Epoch 5 | Batch 1200/3125 | Loss: 2.3249
Epoch 5 | Batch 1250/3125 | Loss: 2.1413
Epoch 5 | Batch 1300/3125 | Loss: 2.3854
Epoch 5 | Batch 1350/3125 | Loss: 2.2431
Epoch 5 | Batch 1400/3125 | Loss: 2.2861
Epoch 5 | Batch 1450/3125 | Loss: 2.1139
Epoch 5 | Batch 1500/3125 | Loss: 2.3287
Epoch 5 | Batch 1550/3125 | Loss: 2.1983
Epoch 5 | Batch 1600/3125 | Loss: 2.1651
Epoch 5 | Batch 1650/3125 | Loss: 2.3469
Epoch 5 | Batch 1700/3125 | Loss: 2.4421
Epoch 5 | Batch 1750/3125 | Loss: 2.2833
Epoch 5 | Batch 1800/3125 | Loss: 2.2744
Epoch 5 | Batch 1850/3125 | Loss: 2.1844
Epoch 5 | Batch 1900/3125 | Loss: 2.3716
Epoch 5 | Batch 1950/3125 | Loss: 2.2016
Epoch 5 | Batch 2000/3125 | Loss: 2.2750
Epoch 5 | Batch 2050/3125 | Loss: 2.4178
Epoch 5 | Batch 2100/3125 | Loss: 2.0827
Epoch 5 | Batch 2150/3125 | Loss: 2.3290
Epoch 5 | Batch 2200/3125 | Loss: 2.3381
Epoch 5 | Batch 2250/3125 | Loss: 2.1249
Epoch 5 | Batch 2300/3125 | Loss: 2.1731
Epoch 5 | Batch 2350/3125 | Loss: 2.3278
Epoch 5 | Batch 2400/3125 | Loss: 2.2896
Epoch 5 | Batch 2450/3125 | Loss: 2.2728
Epoch 5 | Batch 2500/3125 | Loss: 2.3089
Epoch 5 | Batch 2550/3125 | Loss: 2.2384
Epoch 5 | Batch 2600/3125 | Loss: 2.1157
Epoch 5 | Batch 2650/3125 | Loss: 2.2098
Epoch 5 | Batch 2700/3125 | Loss: 2.2304
Epoch 5 | Batch 2750/3125 | Loss: 2.4229
Epoch 5 | Batch 2800/3125 | Loss: 2.3061
Epoch 5 | Batch 2850/3125 | Loss: 2.2400
Epoch 5 | Batch 2900/3125 | Loss: 2.2419
Epoch 5 | Batch 2950/3125 | Loss: 2.2407
Epoch 5 | Batch 3000/3125 | Loss: 2.2948
Epoch 5 | Batch 3050/3125 | Loss: 2.2616
Epoch 5 | Batch 3100/3125 | Loss: 2.2014
Epoch 5 | Time: 484.4s | Train: 2.2354 | Val: 4.0505
Epoch 6 | Batch 0/3125 | Loss: 2.0745
Epoch 6 | Batch 50/3125 | Loss: 2.0206
Epoch 6 | Batch 100/3125 | Loss: 2.0950
Epoch 6 | Batch 150/3125 | Loss: 2.0715
Epoch 6 | Batch 200/3125 | Loss: 1.8953
Epoch 6 | Batch 250/3125 | Loss: 1.8086
Epoch 6 | Batch 300/3125 | Loss: 2.1000
Epoch 6 | Batch 350/3125 | Loss: 2.0258
Epoch 6 | Batch 400/3125 | Loss: 2.0013
Epoch 6 | Batch 450/3125 | Loss: 1.8476
Epoch 6 | Batch 500/3125 | Loss: 2.0534
Epoch 6 | Batch 550/3125 | Loss: 2.1342
Epoch 6 | Batch 600/3125 | Loss: 2.0153
Epoch 6 | Batch 650/3125 | Loss: 2.1638
Epoch 6 | Batch 700/3125 | Loss: 2.0208
Epoch 6 | Batch 750/3125 | Loss: 2.0423
Epoch 6 | Batch 800/3125 | Loss: 2.0318
Epoch 6 | Batch 850/3125 | Loss: 2.0743
Epoch 6 | Batch 900/3125 | Loss: 2.0867
Epoch 6 | Batch 950/3125 | Loss: 2.0760
Epoch 6 | Batch 1000/3125 | Loss: 2.0861
Epoch 6 | Batch 1050/3125 | Loss: 2.1403
Epoch 6 | Batch 1100/3125 | Loss: 2.0975
Epoch 6 | Batch 1150/3125 | Loss: 2.3704
Epoch 6 | Batch 1200/3125 | Loss: 2.0690
Epoch 6 | Batch 1250/3125 | Loss: 2.0969
Epoch 6 | Batch 1300/3125 | Loss: 1.9902
Epoch 6 | Batch 1350/3125 | Loss: 2.2047
Epoch 6 | Batch 1400/3125 | Loss: 2.1622
Epoch 6 | Batch 1450/3125 | Loss: 2.0902
Epoch 6 | Batch 1500/3125 | Loss: 2.0265
Epoch 6 | Batch 1550/3125 | Loss: 1.7429
Epoch 6 | Batch 1600/3125 | Loss: 2.1183
Epoch 6 | Batch 1650/3125 | Loss: 2.2560
Epoch 6 | Batch 1700/3125 | Loss: 2.1675
Epoch 6 | Batch 1750/3125 | Loss: 2.0070
Epoch 6 | Batch 1800/3125 | Loss: 2.1125
Epoch 6 | Batch 1850/3125 | Loss: 2.2095
Epoch 6 | Batch 1900/3125 | Loss: 1.9988
Epoch 6 | Batch 1950/3125 | Loss: 2.1211
Epoch 6 | Batch 2000/3125 | Loss: 2.0779
Epoch 6 | Batch 2050/3125 | Loss: 2.0047
Epoch 6 | Batch 2100/3125 | Loss: 2.4168
Epoch 6 | Batch 2150/3125 | Loss: 2.1767
Epoch 6 | Batch 2200/3125 | Loss: 2.0646
Epoch 6 | Batch 2250/3125 | Loss: 2.1381
Epoch 6 | Batch 2300/3125 | Loss: 2.1257
Epoch 6 | Batch 2350/3125 | Loss: 2.1643
Epoch 6 | Batch 2400/3125 | Loss: 2.1819
Epoch 6 | Batch 2450/3125 | Loss: 2.1322
Epoch 6 | Batch 2500/3125 | Loss: 2.1591
Epoch 6 | Batch 2550/3125 | Loss: 2.2601
Epoch 6 | Batch 2600/3125 | Loss: 2.3249
Epoch 6 | Batch 2650/3125 | Loss: 2.3075
Epoch 6 | Batch 2700/3125 | Loss: 2.1496
Epoch 6 | Batch 2750/3125 | Loss: 2.2564
Epoch 6 | Batch 2800/3125 | Loss: 2.2138
Epoch 6 | Batch 2850/3125 | Loss: 2.1261
Epoch 6 | Batch 2900/3125 | Loss: 2.2837
Epoch 6 | Batch 2950/3125 | Loss: 2.1182
Epoch 6 | Batch 3000/3125 | Loss: 2.1346
Epoch 6 | Batch 3050/3125 | Loss: 2.2136
Epoch 6 | Batch 3100/3125 | Loss: 2.2218
Epoch 6 | Time: 480.9s | Train: 2.1142 | Val: 4.1597
Epoch 7 | Batch 0/3125 | Loss: 1.8649
Epoch 7 | Batch 50/3125 | Loss: 2.0194
Epoch 7 | Batch 100/3125 | Loss: 1.9045
Epoch 7 | Batch 150/3125 | Loss: 1.5424
Epoch 7 | Batch 200/3125 | Loss: 1.8648
Epoch 7 | Batch 250/3125 | Loss: 1.9706
Epoch 7 | Batch 300/3125 | Loss: 1.8385
Epoch 7 | Batch 350/3125 | Loss: 1.7876
Epoch 7 | Batch 400/3125 | Loss: 1.9858
Epoch 7 | Batch 450/3125 | Loss: 1.9104
Epoch 7 | Batch 500/3125 | Loss: 2.1102
Epoch 7 | Batch 550/3125 | Loss: 1.8253
Epoch 7 | Batch 600/3125 | Loss: 1.7637
Epoch 7 | Batch 650/3125 | Loss: 1.8712
Epoch 7 | Batch 700/3125 | Loss: 1.9097
Epoch 7 | Batch 750/3125 | Loss: 1.9426
Epoch 7 | Batch 800/3125 | Loss: 2.0420
Epoch 7 | Batch 850/3125 | Loss: 1.8397
Epoch 7 | Batch 900/3125 | Loss: 2.0690
Epoch 7 | Batch 950/3125 | Loss: 2.0090
Epoch 7 | Batch 1000/3125 | Loss: 1.8615
Epoch 7 | Batch 1050/3125 | Loss: 2.0926
Epoch 7 | Batch 1100/3125 | Loss: 1.8136
Epoch 7 | Batch 1150/3125 | Loss: 1.8396
Epoch 7 | Batch 1200/3125 | Loss: 2.1253
Epoch 7 | Batch 1250/3125 | Loss: 1.9493
Epoch 7 | Batch 1300/3125 | Loss: 1.9927
Epoch 7 | Batch 1350/3125 | Loss: 1.9353
Epoch 7 | Batch 1400/3125 | Loss: 1.9802
Epoch 7 | Batch 1450/3125 | Loss: 1.9720
Epoch 7 | Batch 1500/3125 | Loss: 1.9616
Epoch 7 | Batch 1550/3125 | Loss: 1.8462
Epoch 7 | Batch 1600/3125 | Loss: 1.9188
Epoch 7 | Batch 1650/3125 | Loss: 1.9114
Epoch 7 | Batch 1700/3125 | Loss: 1.9555
Epoch 7 | Batch 1750/3125 | Loss: 2.0177
Epoch 7 | Batch 1800/3125 | Loss: 1.9591
Epoch 7 | Batch 1850/3125 | Loss: 2.2018
Epoch 7 | Batch 1900/3125 | Loss: 2.0495
Epoch 7 | Batch 1950/3125 | Loss: 1.8173
Epoch 7 | Batch 2000/3125 | Loss: 1.8453
Epoch 7 | Batch 2050/3125 | Loss: 2.1398
Epoch 7 | Batch 2100/3125 | Loss: 2.1976
Epoch 7 | Batch 2150/3125 | Loss: 2.2366
Epoch 7 | Batch 2200/3125 | Loss: 1.9572
Epoch 7 | Batch 2250/3125 | Loss: 2.2332
Epoch 7 | Batch 2300/3125 | Loss: 2.0025
Epoch 7 | Batch 2350/3125 | Loss: 2.0533
Epoch 7 | Batch 2400/3125 | Loss: 2.1165
Epoch 7 | Batch 2450/3125 | Loss: 2.1492
Epoch 7 | Batch 2500/3125 | Loss: 2.2012
Epoch 7 | Batch 2550/3125 | Loss: 1.9277
Epoch 7 | Batch 2600/3125 | Loss: 2.1894
Epoch 7 | Batch 2650/3125 | Loss: 2.0635
Epoch 7 | Batch 2700/3125 | Loss: 1.9269
Epoch 7 | Batch 2750/3125 | Loss: 2.2470
Epoch 7 | Batch 2800/3125 | Loss: 2.0920
Epoch 7 | Batch 2850/3125 | Loss: 1.9175
Epoch 7 | Batch 2900/3125 | Loss: 2.0596
Epoch 7 | Batch 2950/3125 | Loss: 2.1481
Epoch 7 | Batch 3000/3125 | Loss: 1.9486
Epoch 7 | Batch 3050/3125 | Loss: 1.8529
Epoch 7 | Batch 3100/3125 | Loss: 2.0026
Epoch 7 | Time: 481.9s | Train: 1.9985 | Val: 4.2785
Epoch 8 | Batch 0/3125 | Loss: 1.8139
Epoch 8 | Batch 50/3125 | Loss: 1.7858
Epoch 8 | Batch 100/3125 | Loss: 2.0167
Epoch 8 | Batch 150/3125 | Loss: 1.6714
Epoch 8 | Batch 200/3125 | Loss: 1.8162
Epoch 8 | Batch 250/3125 | Loss: 1.6900
Epoch 8 | Batch 300/3125 | Loss: 1.8947
Epoch 8 | Batch 350/3125 | Loss: 2.0227
Epoch 8 | Batch 400/3125 | Loss: 1.8921
Epoch 8 | Batch 450/3125 | Loss: 1.8313
Epoch 8 | Batch 500/3125 | Loss: 1.8829
Epoch 8 | Batch 550/3125 | Loss: 1.7503
Epoch 8 | Batch 600/3125 | Loss: 2.0351
Epoch 8 | Batch 650/3125 | Loss: 1.8529
Epoch 8 | Batch 700/3125 | Loss: 1.9113
Epoch 8 | Batch 750/3125 | Loss: 2.0739
Epoch 8 | Batch 800/3125 | Loss: 1.9225
Epoch 8 | Batch 850/3125 | Loss: 1.9157
Epoch 8 | Batch 900/3125 | Loss: 1.9038
Epoch 8 | Batch 950/3125 | Loss: 2.0892
Epoch 8 | Batch 1000/3125 | Loss: 1.9570
Epoch 8 | Batch 1050/3125 | Loss: 1.9135
Epoch 8 | Batch 1100/3125 | Loss: 1.6004
Epoch 8 | Batch 1150/3125 | Loss: 1.7951
Epoch 8 | Batch 1200/3125 | Loss: 1.8064
Epoch 8 | Batch 1250/3125 | Loss: 2.1078
Epoch 8 | Batch 1300/3125 | Loss: 2.0626
Epoch 8 | Batch 1350/3125 | Loss: 1.7307
Epoch 8 | Batch 1400/3125 | Loss: 1.8430
Epoch 8 | Batch 1450/3125 | Loss: 1.8152
Epoch 8 | Batch 1500/3125 | Loss: 1.7983
Epoch 8 | Batch 1550/3125 | Loss: 1.8947
Epoch 8 | Batch 1600/3125 | Loss: 2.0096
Epoch 8 | Batch 1650/3125 | Loss: 1.7783
Epoch 8 | Batch 1700/3125 | Loss: 1.8290
Epoch 8 | Batch 1750/3125 | Loss: 1.7576
Epoch 8 | Batch 1800/3125 | Loss: 1.7765
Epoch 8 | Batch 1850/3125 | Loss: 1.9124
Epoch 8 | Batch 1900/3125 | Loss: 1.9111
Epoch 8 | Batch 1950/3125 | Loss: 2.0224
Epoch 8 | Batch 2000/3125 | Loss: 2.0128
Epoch 8 | Batch 2050/3125 | Loss: 1.9692
Epoch 8 | Batch 2100/3125 | Loss: 1.9111
Epoch 8 | Batch 2150/3125 | Loss: 1.9151
Epoch 8 | Batch 2200/3125 | Loss: 1.9094
Epoch 8 | Batch 2250/3125 | Loss: 1.8234
Epoch 8 | Batch 2300/3125 | Loss: 1.9521
Epoch 8 | Batch 2350/3125 | Loss: 1.8982
Epoch 8 | Batch 2400/3125 | Loss: 1.8649
Epoch 8 | Batch 2450/3125 | Loss: 1.8779
Epoch 8 | Batch 2500/3125 | Loss: 1.8620
Epoch 8 | Batch 2550/3125 | Loss: 1.9102
Epoch 8 | Batch 2600/3125 | Loss: 2.0183
Epoch 8 | Batch 2650/3125 | Loss: 1.8056
Epoch 8 | Batch 2700/3125 | Loss: 1.7925
Epoch 8 | Batch 2750/3125 | Loss: 1.9530
Epoch 8 | Batch 2800/3125 | Loss: 2.0897
Epoch 8 | Batch 2850/3125 | Loss: 1.7832
Epoch 8 | Batch 2900/3125 | Loss: 2.0534
Epoch 8 | Batch 2950/3125 | Loss: 2.0060
Epoch 8 | Batch 3000/3125 | Loss: 1.9719
Epoch 8 | Batch 3050/3125 | Loss: 1.9471
Epoch 8 | Batch 3100/3125 | Loss: 1.9831
Epoch 8 | Time: 474.6s | Train: 1.8894 | Val: 4.4019
Epoch 9 | Batch 0/3125 | Loss: 1.8404
Epoch 9 | Batch 50/3125 | Loss: 1.5433
Epoch 9 | Batch 100/3125 | Loss: 1.5417
Epoch 9 | Batch 150/3125 | Loss: 1.7694
Epoch 9 | Batch 200/3125 | Loss: 1.6391
Epoch 9 | Batch 250/3125 | Loss: 1.6866
Epoch 9 | Batch 300/3125 | Loss: 1.7982
Epoch 9 | Batch 350/3125 | Loss: 1.8781
Epoch 9 | Batch 400/3125 | Loss: 1.7742
Epoch 9 | Batch 450/3125 | Loss: 1.7129
Epoch 9 | Batch 500/3125 | Loss: 1.8012
Epoch 9 | Batch 550/3125 | Loss: 1.5487
Epoch 9 | Batch 600/3125 | Loss: 1.6897
Epoch 9 | Batch 650/3125 | Loss: 1.6128
Epoch 9 | Batch 700/3125 | Loss: 1.8122
Epoch 9 | Batch 750/3125 | Loss: 1.7846
Epoch 9 | Batch 800/3125 | Loss: 1.7043
Epoch 9 | Batch 850/3125 | Loss: 1.7636
Epoch 9 | Batch 900/3125 | Loss: 1.7666
Epoch 9 | Batch 950/3125 | Loss: 1.7385
Epoch 9 | Batch 1000/3125 | Loss: 1.8520
Epoch 9 | Batch 1050/3125 | Loss: 1.8157
Epoch 9 | Batch 1100/3125 | Loss: 1.7203
Epoch 9 | Batch 1150/3125 | Loss: 1.7766
Epoch 9 | Batch 1200/3125 | Loss: 1.8501
Epoch 9 | Batch 1250/3125 | Loss: 1.5604
Epoch 9 | Batch 1300/3125 | Loss: 1.8886
Epoch 9 | Batch 1350/3125 | Loss: 1.8080
Epoch 9 | Batch 1400/3125 | Loss: 1.7358
Epoch 9 | Batch 1450/3125 | Loss: 1.6225
Epoch 9 | Batch 1500/3125 | Loss: 1.8867
Epoch 9 | Batch 1550/3125 | Loss: 1.8585
Epoch 9 | Batch 1600/3125 | Loss: 1.7977
Epoch 9 | Batch 1650/3125 | Loss: 1.7878
Epoch 9 | Batch 1700/3125 | Loss: 1.6477
Epoch 9 | Batch 1750/3125 | Loss: 1.6566
Epoch 9 | Batch 1800/3125 | Loss: 1.8617
Epoch 9 | Batch 1850/3125 | Loss: 1.8736
Epoch 9 | Batch 1900/3125 | Loss: 1.7747
Epoch 9 | Batch 1950/3125 | Loss: 1.6979
Epoch 9 | Batch 2000/3125 | Loss: 1.8477
Epoch 9 | Batch 2050/3125 | Loss: 1.8837
Epoch 9 | Batch 2100/3125 | Loss: 1.8242
Epoch 9 | Batch 2150/3125 | Loss: 1.7879
Epoch 9 | Batch 2200/3125 | Loss: 1.7348
Epoch 9 | Batch 2250/3125 | Loss: 1.9425
Epoch 9 | Batch 2300/3125 | Loss: 1.9254
Epoch 9 | Batch 2350/3125 | Loss: 1.8553
Epoch 9 | Batch 2400/3125 | Loss: 1.9004
Epoch 9 | Batch 2450/3125 | Loss: 1.8819
Epoch 9 | Batch 2500/3125 | Loss: 1.8364
Epoch 9 | Batch 2550/3125 | Loss: 1.8507
Epoch 9 | Batch 2600/3125 | Loss: 1.6498
Epoch 9 | Batch 2650/3125 | Loss: 1.9046
Epoch 9 | Batch 2700/3125 | Loss: 1.8547
Epoch 9 | Batch 2750/3125 | Loss: 1.7654
Epoch 9 | Batch 2800/3125 | Loss: 1.7842
Epoch 9 | Batch 2850/3125 | Loss: 1.8777
Epoch 9 | Batch 2900/3125 | Loss: 1.7679
Epoch 9 | Batch 2950/3125 | Loss: 1.8965
Epoch 9 | Batch 3000/3125 | Loss: 1.8760
Epoch 9 | Batch 3050/3125 | Loss: 1.6768
Epoch 9 | Batch 3100/3125 | Loss: 1.6011
Epoch 9 | Time: 476.6s | Train: 1.7860 | Val: 4.5632

Done! Best Val Loss: 3.6344
