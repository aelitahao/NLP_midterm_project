Loading train.pt and valid.pt from data/processed_100k
Train samples: 99392, Valid samples: 486
Parameters: 44,797,712
Training transformer on cuda
/data/250010067/courses/NLP/NLP_midterm_project/train.py:100: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with autocast(enabled=self.use_amp):
Epoch 0 | Batch 0/1553 | Loss: 9.2712
Epoch 0 | Batch 100/1553 | Loss: 8.6149
Epoch 0 | Batch 200/1553 | Loss: 8.2958
Epoch 0 | Batch 300/1553 | Loss: 7.9143
Epoch 0 | Batch 400/1553 | Loss: 7.4245
Epoch 0 | Batch 500/1553 | Loss: 6.9914
Epoch 0 | Batch 600/1553 | Loss: 6.5374
Epoch 0 | Batch 700/1553 | Loss: 6.2810
Epoch 0 | Batch 800/1553 | Loss: 6.1398
Epoch 0 | Batch 900/1553 | Loss: 6.0697
Epoch 0 | Batch 1000/1553 | Loss: 5.9866
Epoch 0 | Batch 1100/1553 | Loss: 5.8208
Epoch 0 | Batch 1200/1553 | Loss: 5.8261
Epoch 0 | Batch 1300/1553 | Loss: 5.6603
Epoch 0 | Batch 1400/1553 | Loss: 5.6698
Epoch 0 | Batch 1500/1553 | Loss: 5.8156
/data/250010067/courses/NLP/NLP_midterm_project/train.py:141: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with autocast(enabled=self.use_amp):
Epoch 0 | Time: 37.4s | Train Loss: 6.6877 | Val Loss: 5.6804
New best model! Val Loss: 5.6804
Epoch 1 | Batch 0/1553 | Loss: 5.5422
Epoch 1 | Batch 100/1553 | Loss: 5.4630
Epoch 1 | Batch 200/1553 | Loss: 5.5143
Epoch 1 | Batch 300/1553 | Loss: 5.4295
Epoch 1 | Batch 400/1553 | Loss: 5.3693
Epoch 1 | Batch 500/1553 | Loss: 5.3508
Epoch 1 | Batch 600/1553 | Loss: 5.3764
Epoch 1 | Batch 700/1553 | Loss: 5.1994
Epoch 1 | Batch 800/1553 | Loss: 5.1191
Epoch 1 | Batch 900/1553 | Loss: 4.9923
Epoch 1 | Batch 1000/1553 | Loss: 5.0888
Epoch 1 | Batch 1100/1553 | Loss: 4.8991
Epoch 1 | Batch 1200/1553 | Loss: 4.8465
Epoch 1 | Batch 1300/1553 | Loss: 4.8425
Epoch 1 | Batch 1400/1553 | Loss: 4.8629
Epoch 1 | Batch 1500/1553 | Loss: 4.7249
Epoch 1 | Time: 39.9s | Train Loss: 5.1477 | Val Loss: 4.9937
New best model! Val Loss: 4.9937
Epoch 2 | Batch 0/1553 | Loss: 4.7941
Epoch 2 | Batch 100/1553 | Loss: 4.6549
Epoch 2 | Batch 200/1553 | Loss: 4.7103
Epoch 2 | Batch 300/1553 | Loss: 4.5257
Epoch 2 | Batch 400/1553 | Loss: 4.4462
Epoch 2 | Batch 500/1553 | Loss: 4.4718
Epoch 2 | Batch 600/1553 | Loss: 4.5224
Epoch 2 | Batch 700/1553 | Loss: 4.2312
Epoch 2 | Batch 800/1553 | Loss: 4.4197
Epoch 2 | Batch 900/1553 | Loss: 4.3104
Epoch 2 | Batch 1000/1553 | Loss: 4.1017
Epoch 2 | Batch 1100/1553 | Loss: 4.0651
Epoch 2 | Batch 1200/1553 | Loss: 4.0846
Epoch 2 | Batch 1300/1553 | Loss: 4.1716
Epoch 2 | Batch 1400/1553 | Loss: 4.1522
Epoch 2 | Batch 1500/1553 | Loss: 4.0749
Epoch 2 | Time: 39.7s | Train Loss: 4.3289 | Val Loss: 4.4728
New best model! Val Loss: 4.4728
Epoch 3 | Batch 0/1553 | Loss: 3.9670
Epoch 3 | Batch 100/1553 | Loss: 3.7663
Epoch 3 | Batch 200/1553 | Loss: 3.9105
Epoch 3 | Batch 300/1553 | Loss: 3.7903
Epoch 3 | Batch 400/1553 | Loss: 3.8750
Epoch 3 | Batch 500/1553 | Loss: 3.6948
Epoch 3 | Batch 600/1553 | Loss: 3.7848
Epoch 3 | Batch 700/1553 | Loss: 3.7439
Epoch 3 | Batch 800/1553 | Loss: 3.7611
Epoch 3 | Batch 900/1553 | Loss: 3.7031
Epoch 3 | Batch 1000/1553 | Loss: 3.6883
Epoch 3 | Batch 1100/1553 | Loss: 3.6823
Epoch 3 | Batch 1200/1553 | Loss: 3.5569
Epoch 3 | Batch 1300/1553 | Loss: 3.5699
Epoch 3 | Batch 1400/1553 | Loss: 3.6069
Epoch 3 | Batch 1500/1553 | Loss: 3.5903
Epoch 3 | Time: 40.2s | Train Loss: 3.6991 | Val Loss: 4.1740
New best model! Val Loss: 4.1740
Epoch 4 | Batch 0/1553 | Loss: 3.2843
Epoch 4 | Batch 100/1553 | Loss: 3.3811
Epoch 4 | Batch 200/1553 | Loss: 3.3433
Epoch 4 | Batch 300/1553 | Loss: 3.4490
Epoch 4 | Batch 400/1553 | Loss: 3.3518
Epoch 4 | Batch 500/1553 | Loss: 3.3232
Epoch 4 | Batch 600/1553 | Loss: 3.2280
Epoch 4 | Batch 700/1553 | Loss: 3.4284
Epoch 4 | Batch 800/1553 | Loss: 3.2297
Epoch 4 | Batch 900/1553 | Loss: 3.2131
Epoch 4 | Batch 1000/1553 | Loss: 3.4070
Epoch 4 | Batch 1100/1553 | Loss: 3.2635
Epoch 4 | Batch 1200/1553 | Loss: 3.2533
Epoch 4 | Batch 1300/1553 | Loss: 3.1831
Epoch 4 | Batch 1400/1553 | Loss: 2.9561
Epoch 4 | Batch 1500/1553 | Loss: 3.1200
Epoch 4 | Time: 39.9s | Train Loss: 3.2856 | Val Loss: 3.9852
New best model! Val Loss: 3.9852
Epoch 5 | Batch 0/1553 | Loss: 3.0055
Epoch 5 | Batch 100/1553 | Loss: 3.0274
Epoch 5 | Batch 200/1553 | Loss: 2.9271
Epoch 5 | Batch 300/1553 | Loss: 3.0969
Epoch 5 | Batch 400/1553 | Loss: 2.8510
Epoch 5 | Batch 500/1553 | Loss: 2.9835
Epoch 5 | Batch 600/1553 | Loss: 3.0359
Epoch 5 | Batch 700/1553 | Loss: 2.8309
Epoch 5 | Batch 800/1553 | Loss: 3.0815
Epoch 5 | Batch 900/1553 | Loss: 2.9465
Epoch 5 | Batch 1000/1553 | Loss: 3.0303
Epoch 5 | Batch 1100/1553 | Loss: 2.9179
Epoch 5 | Batch 1200/1553 | Loss: 2.8985
Epoch 5 | Batch 1300/1553 | Loss: 3.0728
Epoch 5 | Batch 1400/1553 | Loss: 2.9106
Epoch 5 | Batch 1500/1553 | Loss: 3.1109
Epoch 5 | Time: 39.9s | Train Loss: 3.0020 | Val Loss: 3.8522
New best model! Val Loss: 3.8522
Epoch 6 | Batch 0/1553 | Loss: 2.9852
Epoch 6 | Batch 100/1553 | Loss: 2.8824
Epoch 6 | Batch 200/1553 | Loss: 3.0968
Epoch 6 | Batch 300/1553 | Loss: 2.8863
Epoch 6 | Batch 400/1553 | Loss: 2.8363
Epoch 6 | Batch 500/1553 | Loss: 2.7546
Epoch 6 | Batch 600/1553 | Loss: 2.7847
Epoch 6 | Batch 700/1553 | Loss: 2.7851
Epoch 6 | Batch 800/1553 | Loss: 2.8285
Epoch 6 | Batch 900/1553 | Loss: 2.8120
Epoch 6 | Batch 1000/1553 | Loss: 2.7528
Epoch 6 | Batch 1100/1553 | Loss: 2.7182
Epoch 6 | Batch 1200/1553 | Loss: 2.9571
Epoch 6 | Batch 1300/1553 | Loss: 2.8307
Epoch 6 | Batch 1400/1553 | Loss: 2.9617
Epoch 6 | Batch 1500/1553 | Loss: 2.7232
Epoch 6 | Time: 36.7s | Train Loss: 2.7919 | Val Loss: 3.7902
New best model! Val Loss: 3.7902
Epoch 7 | Batch 0/1553 | Loss: 2.7218
Epoch 7 | Batch 100/1553 | Loss: 2.6413
Epoch 7 | Batch 200/1553 | Loss: 2.7508
Epoch 7 | Batch 300/1553 | Loss: 2.6949
Epoch 7 | Batch 400/1553 | Loss: 2.6299
Epoch 7 | Batch 500/1553 | Loss: 2.6407
Epoch 7 | Batch 600/1553 | Loss: 2.6753
Epoch 7 | Batch 700/1553 | Loss: 2.6929
Epoch 7 | Batch 800/1553 | Loss: 2.5938
Epoch 7 | Batch 900/1553 | Loss: 2.8218
Epoch 7 | Batch 1000/1553 | Loss: 2.7794
Epoch 7 | Batch 1100/1553 | Loss: 2.6991
Epoch 7 | Batch 1200/1553 | Loss: 2.6422
Epoch 7 | Batch 1300/1553 | Loss: 2.6801
Epoch 7 | Batch 1400/1553 | Loss: 2.4596
Epoch 7 | Batch 1500/1553 | Loss: 2.6919
Epoch 7 | Time: 36.6s | Train Loss: 2.6325 | Val Loss: 3.7353
New best model! Val Loss: 3.7353
Epoch 8 | Batch 0/1553 | Loss: 2.4935
Epoch 8 | Batch 100/1553 | Loss: 2.4303
Epoch 8 | Batch 200/1553 | Loss: 2.5890
Epoch 8 | Batch 300/1553 | Loss: 2.5427
Epoch 8 | Batch 400/1553 | Loss: 2.4457
Epoch 8 | Batch 500/1553 | Loss: 2.4992
Epoch 8 | Batch 600/1553 | Loss: 2.4415
Epoch 8 | Batch 700/1553 | Loss: 2.5311
Epoch 8 | Batch 800/1553 | Loss: 2.4439
Epoch 8 | Batch 900/1553 | Loss: 2.4419
Epoch 8 | Batch 1000/1553 | Loss: 2.5669
Epoch 8 | Batch 1100/1553 | Loss: 2.3854
Epoch 8 | Batch 1200/1553 | Loss: 2.4633
Epoch 8 | Batch 1300/1553 | Loss: 2.4700
Epoch 8 | Batch 1400/1553 | Loss: 2.2563
Epoch 8 | Batch 1500/1553 | Loss: 2.5707
Epoch 8 | Time: 36.6s | Train Loss: 2.5047 | Val Loss: 3.6807
New best model! Val Loss: 3.6807
Epoch 9 | Batch 0/1553 | Loss: 2.4221
Epoch 9 | Batch 100/1553 | Loss: 2.5879
Epoch 9 | Batch 200/1553 | Loss: 2.3756
Epoch 9 | Batch 300/1553 | Loss: 2.3519
Epoch 9 | Batch 400/1553 | Loss: 2.2061
Epoch 9 | Batch 500/1553 | Loss: 2.3036
Epoch 9 | Batch 600/1553 | Loss: 2.3014
Epoch 9 | Batch 700/1553 | Loss: 2.4101
Epoch 9 | Batch 800/1553 | Loss: 2.4212
Epoch 9 | Batch 900/1553 | Loss: 2.5605
Epoch 9 | Batch 1000/1553 | Loss: 2.4298
Epoch 9 | Batch 1100/1553 | Loss: 2.3497
Epoch 9 | Batch 1200/1553 | Loss: 2.6645
Epoch 9 | Batch 1300/1553 | Loss: 2.3423
Epoch 9 | Batch 1400/1553 | Loss: 2.5045
Epoch 9 | Batch 1500/1553 | Loss: 2.4726
Epoch 9 | Time: 36.8s | Train Loss: 2.4008 | Val Loss: 3.6403
New best model! Val Loss: 3.6403
Epoch 10 | Batch 0/1553 | Loss: 2.3269
Epoch 10 | Batch 100/1553 | Loss: 2.1749
Epoch 10 | Batch 200/1553 | Loss: 2.3798
Epoch 10 | Batch 300/1553 | Loss: 2.4511
Epoch 10 | Batch 400/1553 | Loss: 2.2800
Epoch 10 | Batch 500/1553 | Loss: 2.4875
Epoch 10 | Batch 600/1553 | Loss: 2.3732
Epoch 10 | Batch 700/1553 | Loss: 2.3454
Epoch 10 | Batch 800/1553 | Loss: 2.1351
Epoch 10 | Batch 900/1553 | Loss: 2.3265
Epoch 10 | Batch 1000/1553 | Loss: 2.3650
Epoch 10 | Batch 1100/1553 | Loss: 2.2375
Epoch 10 | Batch 1200/1553 | Loss: 2.2723
Epoch 10 | Batch 1300/1553 | Loss: 2.3592
Epoch 10 | Batch 1400/1553 | Loss: 2.3624
Epoch 10 | Batch 1500/1553 | Loss: 2.3621
Epoch 10 | Time: 36.0s | Train Loss: 2.3115 | Val Loss: 3.6179
New best model! Val Loss: 3.6179
Epoch 11 | Batch 0/1553 | Loss: 2.2131
Epoch 11 | Batch 100/1553 | Loss: 2.2502
Epoch 11 | Batch 200/1553 | Loss: 2.2667
Epoch 11 | Batch 300/1553 | Loss: 2.1730
Epoch 11 | Batch 400/1553 | Loss: 2.1356
Epoch 11 | Batch 500/1553 | Loss: 2.1237
Epoch 11 | Batch 600/1553 | Loss: 2.4684
Epoch 11 | Batch 700/1553 | Loss: 2.1027
Epoch 11 | Batch 800/1553 | Loss: 2.2156
Epoch 11 | Batch 900/1553 | Loss: 2.2162
Epoch 11 | Batch 1000/1553 | Loss: 2.2199
Epoch 11 | Batch 1100/1553 | Loss: 2.3015
Epoch 11 | Batch 1200/1553 | Loss: 2.3576
Epoch 11 | Batch 1300/1553 | Loss: 2.3078
Epoch 11 | Batch 1400/1553 | Loss: 2.2942
Epoch 11 | Batch 1500/1553 | Loss: 2.1705
Epoch 11 | Time: 36.2s | Train Loss: 2.2337 | Val Loss: 3.6114
New best model! Val Loss: 3.6114
Epoch 12 | Batch 0/1553 | Loss: 2.3228
Epoch 12 | Batch 100/1553 | Loss: 2.1637
Epoch 12 | Batch 200/1553 | Loss: 2.1402
Epoch 12 | Batch 300/1553 | Loss: 2.3031
Epoch 12 | Batch 400/1553 | Loss: 2.1842
Epoch 12 | Batch 500/1553 | Loss: 2.1150
Epoch 12 | Batch 600/1553 | Loss: 2.1389
Epoch 12 | Batch 700/1553 | Loss: 2.0118
Epoch 12 | Batch 800/1553 | Loss: 2.2625
Epoch 12 | Batch 900/1553 | Loss: 2.1900
Epoch 12 | Batch 1000/1553 | Loss: 2.1347
Epoch 12 | Batch 1100/1553 | Loss: 2.2903
Epoch 12 | Batch 1200/1553 | Loss: 2.1886
Epoch 12 | Batch 1300/1553 | Loss: 2.1956
Epoch 12 | Batch 1400/1553 | Loss: 2.1818
Epoch 12 | Batch 1500/1553 | Loss: 2.1726
Epoch 12 | Time: 36.5s | Train Loss: 2.1655 | Val Loss: 3.5817
New best model! Val Loss: 3.5817
Epoch 13 | Batch 0/1553 | Loss: 2.2705
Epoch 13 | Batch 100/1553 | Loss: 2.0163
Epoch 13 | Batch 200/1553 | Loss: 2.0306
Epoch 13 | Batch 300/1553 | Loss: 2.1275
Epoch 13 | Batch 400/1553 | Loss: 2.0617
Epoch 13 | Batch 500/1553 | Loss: 2.1658
Epoch 13 | Batch 600/1553 | Loss: 1.9727
Epoch 13 | Batch 700/1553 | Loss: 2.0268
Epoch 13 | Batch 800/1553 | Loss: 2.2769
Epoch 13 | Batch 900/1553 | Loss: 2.0374
Epoch 13 | Batch 1000/1553 | Loss: 2.0464
Epoch 13 | Batch 1100/1553 | Loss: 2.0940
Epoch 13 | Batch 1200/1553 | Loss: 2.0490
Epoch 13 | Batch 1300/1553 | Loss: 2.1562
Epoch 13 | Batch 1400/1553 | Loss: 2.2123
Epoch 13 | Batch 1500/1553 | Loss: 2.2031
Epoch 13 | Time: 36.1s | Train Loss: 2.1039 | Val Loss: 3.6162
Epoch 14 | Batch 0/1553 | Loss: 2.1271
Epoch 14 | Batch 100/1553 | Loss: 2.0557
Epoch 14 | Batch 200/1553 | Loss: 2.1253
Epoch 14 | Batch 300/1553 | Loss: 1.8585
Epoch 14 | Batch 400/1553 | Loss: 2.0360
Epoch 14 | Batch 500/1553 | Loss: 2.0605
Epoch 14 | Batch 600/1553 | Loss: 1.9921
Epoch 14 | Batch 700/1553 | Loss: 2.0656
Epoch 14 | Batch 800/1553 | Loss: 2.0569
Epoch 14 | Batch 900/1553 | Loss: 2.1144
Epoch 14 | Batch 1000/1553 | Loss: 2.0354
Epoch 14 | Batch 1100/1553 | Loss: 2.0854
Epoch 14 | Batch 1200/1553 | Loss: 2.1110
Epoch 14 | Batch 1300/1553 | Loss: 2.0700
Epoch 14 | Batch 1400/1553 | Loss: 2.1734
Epoch 14 | Batch 1500/1553 | Loss: 2.0566
Epoch 14 | Time: 38.0s | Train Loss: 2.0469 | Val Loss: 3.5882
Epoch 15 | Batch 0/1553 | Loss: 1.9149
Epoch 15 | Batch 100/1553 | Loss: 1.9163
Epoch 15 | Batch 200/1553 | Loss: 2.1671
Epoch 15 | Batch 300/1553 | Loss: 1.9045
Epoch 15 | Batch 400/1553 | Loss: 1.9946
Epoch 15 | Batch 500/1553 | Loss: 2.1331
Epoch 15 | Batch 600/1553 | Loss: 2.0290
Epoch 15 | Batch 700/1553 | Loss: 1.8338
Epoch 15 | Batch 800/1553 | Loss: 1.9771
Epoch 15 | Batch 900/1553 | Loss: 1.9192
Epoch 15 | Batch 1000/1553 | Loss: 2.2201
Epoch 15 | Batch 1100/1553 | Loss: 1.9655
Epoch 15 | Batch 1200/1553 | Loss: 2.0437
Epoch 15 | Batch 1300/1553 | Loss: 2.1949
Epoch 15 | Batch 1400/1553 | Loss: 1.9814
Epoch 15 | Batch 1500/1553 | Loss: 2.0580
Epoch 15 | Time: 36.8s | Train Loss: 1.9955 | Val Loss: 3.5987
Epoch 16 | Batch 0/1553 | Loss: 2.0806
Epoch 16 | Batch 100/1553 | Loss: 2.0483
Epoch 16 | Batch 200/1553 | Loss: 2.1211
Epoch 16 | Batch 300/1553 | Loss: 1.9674
Epoch 16 | Batch 400/1553 | Loss: 1.8983
Epoch 16 | Batch 500/1553 | Loss: 1.9911
Epoch 16 | Batch 600/1553 | Loss: 1.9744
Epoch 16 | Batch 700/1553 | Loss: 1.8320
Epoch 16 | Batch 800/1553 | Loss: 1.8513
Epoch 16 | Batch 900/1553 | Loss: 1.9326
Epoch 16 | Batch 1000/1553 | Loss: 1.8272
Epoch 16 | Batch 1100/1553 | Loss: 2.0401
Epoch 16 | Batch 1200/1553 | Loss: 2.0015
Epoch 16 | Batch 1300/1553 | Loss: 1.9152
Epoch 16 | Batch 1400/1553 | Loss: 2.0715
Epoch 16 | Batch 1500/1553 | Loss: 1.9597
Epoch 16 | Time: 35.8s | Train Loss: 1.9476 | Val Loss: 3.6004
Epoch 17 | Batch 0/1553 | Loss: 1.7981
Epoch 17 | Batch 100/1553 | Loss: 1.9669
Epoch 17 | Batch 200/1553 | Loss: 1.9208
Epoch 17 | Batch 300/1553 | Loss: 1.9061
Epoch 17 | Batch 400/1553 | Loss: 1.9124
Epoch 17 | Batch 500/1553 | Loss: 1.8674
Epoch 17 | Batch 600/1553 | Loss: 1.9762
Epoch 17 | Batch 700/1553 | Loss: 1.7212
Epoch 17 | Batch 800/1553 | Loss: 1.9065
Epoch 17 | Batch 900/1553 | Loss: 1.9106
Epoch 17 | Batch 1000/1553 | Loss: 1.7393
Epoch 17 | Batch 1100/1553 | Loss: 2.0325
Epoch 17 | Batch 1200/1553 | Loss: 1.8625
Epoch 17 | Batch 1300/1553 | Loss: 1.8744
Epoch 17 | Batch 1400/1553 | Loss: 1.8990
Epoch 17 | Batch 1500/1553 | Loss: 1.9983
Epoch 17 | Time: 36.0s | Train Loss: 1.9035 | Val Loss: 3.6143
Epoch 18 | Batch 0/1553 | Loss: 1.7169
Epoch 18 | Batch 100/1553 | Loss: 1.8166
Epoch 18 | Batch 200/1553 | Loss: 1.7804
Epoch 18 | Batch 300/1553 | Loss: 1.9681
Epoch 18 | Batch 400/1553 | Loss: 1.8156
Epoch 18 | Batch 500/1553 | Loss: 2.2640
Epoch 18 | Batch 600/1553 | Loss: 1.8436
Epoch 18 | Batch 700/1553 | Loss: 1.8955
Epoch 18 | Batch 800/1553 | Loss: 1.9919
Epoch 18 | Batch 900/1553 | Loss: 1.8844
Epoch 18 | Batch 1000/1553 | Loss: 1.9483
Epoch 18 | Batch 1100/1553 | Loss: 1.6751
Epoch 18 | Batch 1200/1553 | Loss: 1.9902
Epoch 18 | Batch 1300/1553 | Loss: 1.8478
Epoch 18 | Batch 1400/1553 | Loss: 1.8231
Epoch 18 | Batch 1500/1553 | Loss: 1.7445
Epoch 18 | Time: 35.9s | Train Loss: 1.8612 | Val Loss: 3.6226
Epoch 19 | Batch 0/1553 | Loss: 1.8209
Epoch 19 | Batch 100/1553 | Loss: 1.7448
Epoch 19 | Batch 200/1553 | Loss: 1.7913
Epoch 19 | Batch 300/1553 | Loss: 1.9123
Epoch 19 | Batch 400/1553 | Loss: 1.7862
Epoch 19 | Batch 500/1553 | Loss: 1.7193
Epoch 19 | Batch 600/1553 | Loss: 1.8716
Epoch 19 | Batch 700/1553 | Loss: 1.7472
Epoch 19 | Batch 800/1553 | Loss: 1.8143
Epoch 19 | Batch 900/1553 | Loss: 1.7909
Epoch 19 | Batch 1000/1553 | Loss: 1.8572
Epoch 19 | Batch 1100/1553 | Loss: 1.7362
Epoch 19 | Batch 1200/1553 | Loss: 1.6526
Epoch 19 | Batch 1300/1553 | Loss: 1.8632
Epoch 19 | Batch 1400/1553 | Loss: 1.7811
Epoch 19 | Batch 1500/1553 | Loss: 1.7934
Epoch 19 | Time: 36.2s | Train Loss: 1.8223 | Val Loss: 3.6419

Total training time: 00:13:02 (782.9s)
